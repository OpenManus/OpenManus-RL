#!/usr/bin/env python
# -*- coding: utf-8 -*-

"""
Tree of Thought Rollout éªŒè¯è„šæœ¬

è¿™ä¸ªè„šæœ¬ç”¨äºéªŒè¯ Tree of Thought Agent çš„ rollout åŠŸèƒ½æ˜¯å¦æ­£ç¡®å®ç°ï¼Œ
ä¸è¿›è¡Œå®é™…è®­ç»ƒã€‚å®ƒå°†åˆå§‹åŒ–æ‰€éœ€ç»„ä»¶ï¼Œè¿è¡Œå‡ ä¸ª ToT rollout å®ä¾‹ï¼Œ
å¹¶è¯¦ç»†æ‰“å°æ¢ç´¢è¿‡ç¨‹å’Œç»“æœã€‚
"""

import os
import sys
import torch
import argparse
import ray
import json
import time
from typing import Dict, List, Any, Optional, Union, Type
from pathlib import Path

# ç¡®ä¿å¯ä»¥å¯¼å…¥å¿…è¦çš„æ¨¡å—
current_dir = os.path.dirname(os.path.abspath(__file__))
sys.path.insert(0, current_dir)
from enum import Enum
from openmanus_tot import OpenManusToTAgent, ToTConfig
from openmanus_rl.llm_agent.tensor_helper import TensorHelper, TensorConfig

# å¯¼å…¥VERLç›¸å…³ç»„ä»¶
from verl import DataProto
from verl.utils.fs import copy_local_path_from_hdfs
from verl.utils.tokenizer import hf_tokenizer
from verl.single_controller.ray import RayResourcePool, RayWorkerGroup, RayClassWithInitArgs
from verl.workers.fsdp_workers import ActorRolloutRefWorker
from verl.single_controller.ray.base import create_colocated_worker_cls
from verl.single_controller.base import Worker

class Role(Enum):
    ActorRollout = "actor_rollout"
    Critic = "critic"
    RefPolicy = "ref"
    RewardModel = "rm"

class ActorRolloutWorker:
    pass

class CriticWorker:
    pass

class RefPolicyWorker:
    pass

class RewardModelWorker:
    pass

def visualize_trajectory(trajectory, indent=0):
    """
    ä»¥äººç±»å¯è¯»çš„æ–¹å¼å¯è§†åŒ–è½¨è¿¹
    """
    for i, step in enumerate(trajectory):
        role = step.get("from", "unknown")
        if role == "human":
            prefix = "ğŸ§‘ Human:"
        elif role == "gpt":
            prefix = "ğŸ¤– Agent:"
        elif role == "env":
            prefix = "ğŸŒ Env:"
        else:
            prefix = f"â“ {role}:"
        
        # ç¼©è¿›å¹¶æ‰“å°å†…å®¹
        content = step.get("value", "").strip()
        # å¦‚æœå†…å®¹å¾ˆé•¿ï¼Œåªæ˜¾ç¤ºå‰200ä¸ªå­—ç¬¦
        if len(content) > 200:
            content = content[:200] + "... [å†…å®¹å·²æˆªæ–­]"
        
        print(f"{' ' * indent}{prefix} {content}")
        
        # å¦‚æœæœ‰å¥–åŠ±ä¿¡æ¯ï¼Œæ˜¾ç¤ºå®ƒ
        if "reward" in step:
            print(f"{' ' * (indent+4)}ğŸ’° Reward: {step['reward']}")

def validate_tot_agent(model_path: str, env_name: str = "webshop", env_server_base: str = "http://0.0.0.0", env_ports: List[int] = [36001], tot_strategy: str = "BFS", tot_beam_width: int = 3, tot_branches: int = 10, max_turns: int = 5, num_examples: int = 2, debug: bool = False):
    """
    éªŒè¯ Tree of Thought Agent å®ç°

    Args:
        model_path: æ¨¡å‹è·¯å¾„
        env_name: ç¯å¢ƒåç§°
        env_server_base: ç¯å¢ƒæœåŠ¡å™¨åŸºåœ°å€
        env_ports: ç¯å¢ƒæœåŠ¡å™¨ç«¯å£åˆ—è¡¨
        tot_strategy: ToT æœç´¢ç­–ç•¥ ("BFS" æˆ– "DFS")
        tot_beam_width: ToT æŸå®½
        tot_branches: æœ€å¤§åˆ†æ”¯æ•°
        max_turns: æœ€å¤§å›åˆæ•°
        num_examples: è¦æµ‹è¯•çš„ç¤ºä¾‹æ•°é‡
        debug: æ˜¯å¦å¯ç”¨é¢å¤–çš„è°ƒè¯•è¾“å‡º
    """    
    if not ray.is_initialized():
        ray.init(
            num_gpus=torch.cuda.device_count(),
            runtime_env={
                'env_vars': {
                    'TOKENIZERS_PARALLELISM': 'true',
                    'NCCL_DEBUG': 'WARN',
                    'VLLM_LOGGING_LEVEL': 'WARN',
                }
            }
        )

    print(f"ğŸ” éªŒè¯ Tree of Thought Agent å®ç°...")
    print(f"ğŸ“‚ æ¨¡å‹è·¯å¾„: {model_path}")
    print(f"ğŸŒ ç¯å¢ƒ: {env_name}")
    print(f"ğŸ”€ ToT ç­–ç•¥: {tot_strategy}")
    print(f"ğŸ”¢ ToT æŸå®½: {tot_beam_width}")
    print(f"ğŸŒ² æœ€å¤§åˆ†æ”¯æ•°: {tot_branches}")
    print(f"ğŸ”„ æœ€å¤§å›åˆæ•°: {max_turns}")

    # ç¡®ä¿æ¨¡å‹è·¯å¾„å­˜åœ¨
    local_model_path = copy_local_path_from_hdfs(model_path)
    
    print(f"ğŸ”„ åŠ è½½åˆ†è¯å™¨...")
    tokenizer = hf_tokenizer(local_model_path)
    
    role_worker_mapping = {
    Role.ActorRollout: ActorRolloutWorker.options(name="ActorRolloutWorker", max_restarts=3),
    Role.Critic: CriticWorker.options(name="CriticWorker", max_restarts=3),
    Role.RefPolicy: RefPolicyWorker.options(name="RefPolicyWorker", max_restarts=3),
    Role.RewardModel: RewardModelWorker.options(name="RewardModelWorker", max_restarts=3)
}

    class_dict = {
        "actor_rollout": RayClassWithInitArgs(
            cls=role_worker_mapping[Role.ActorRollout],
            config={
                "model": {"path": local_model_path, "enable_gradient_checkpointing": False},
                "rollout": {"name": "vllm", "temperature": 1.0, "top_k": 50, "top_p": 0.95, "gpu_memory_utilization": 0.7}
            },
            role="actor_rollout"
        ),
        "critic": RayClassWithInitArgs(
            cls=role_worker_mapping[Role.Critic],
            config={"critic_type": "simple", "learning_rate": 0.001, "use_gpu": True},
            role="critic"
        ),
        "ref": RayClassWithInitArgs(
            cls=role_worker_mapping[Role.RefPolicy],
            config={"model": {"path": local_model_path, "temperature": 0.7}},
            role="ref"
        ),
        "rm": RayClassWithInitArgs(
            cls=role_worker_mapping[Role.RewardModel],
            config={"reward_fn": None, "scale": 1.0},
            role="rm"
        )
    }


    print(f"ğŸ”„ åˆå§‹åŒ– RayWorkerGroup...")
    worker_dict_cls = create_colocated_worker_cls(class_dict=class_dict)
    wg_dict = RayWorkerGroup(resource_pool=RayResourcePool(process_on_nodes=[torch.cuda.device_count()], use_gpu=True, max_colocate_count=1, name_prefix='tot_validation'), ray_cls_with_init=worker_dict_cls)
    wg_spawn = wg_dict.spawn(prefix_set=['actor_rollout', 'critic', 'ref', 'rm'])
    actor_rollout_wg = wg_spawn['actor_rollout']
    for role, worker in wg_spawn.items():
        worker.init_model()
    
    print("âœ… All workers initialized successfully.")


    # åˆ›å»º ToT é…ç½®
    tot_config = ToTConfig(
        max_turns=max_turns,
        max_start_length=1024,
        max_prompt_length=2048,
        max_response_length=512,
        max_obs_length=1024,
        num_gpus=torch.cuda.device_count(),
        env_name=env_name,
        env_ports=env_ports,
        env_server_base=env_server_base,
        env_data_len=200,
        algorithm_config=None,
        tot_beam_width=tot_beam_width,
        tot_exploration_factor=2,
        tot_max_branches=tot_branches,
        tot_value_guidance=False,  # æš‚æ—¶å…³é—­,å› ä¸ºæˆ‘ä»¬æ²¡æœ‰æä¾›critic
        tot_temperature=1.0,
        tot_search_strategy=tot_strategy
    )
    
    # åˆ›å»º ToT Agent
    print(f"ğŸ”„ åˆ›å»º Tree of Thought Agent...")
    tot_agent = OpenManusToTAgent(
        tokenizer=tokenizer,
        actor_rollout_wg=actor_rollout_wg,
        config=tot_config,
        is_validation=True,
        logger=None
    )
    
    # åˆ›å»ºç®€å•çš„æµ‹è¯•æ ·ä¾‹
    print(f"ğŸ”„ å‡†å¤‡æµ‹è¯•æ ·ä¾‹...")
    
    # ä¸ºç‰¹å®šç¯å¢ƒå‡†å¤‡æç¤º
    if env_name == "webshop":
        prompts = [
            "I'm looking for a stylish black dress that I can wear to a formal dinner.",
            "Find me a comfortable pair of running shoes for men.",
        ]
    elif env_name == "alfworld":
        prompts = [
            "Put a clean mug in the coffee maker.",
            "Find a knife and place it on the counter.",
        ]
    else:
        prompts = [
            f"Help me complete this task in the {env_name} environment.",
            f"I need assistance with the {env_name} environment.",
        ]
    
    # é™åˆ¶ç¤ºä¾‹æ•°é‡
    prompts = prompts[:num_examples]
    
    # åˆ›å»ºæ‰¹æ¬¡
    all_results = []
    
    for i, prompt in enumerate(prompts):
        print(f"\n{'='*80}")
        print(f"ğŸ§ª æµ‹è¯•æ ·ä¾‹ {i+1}/{len(prompts)}: {prompt[:50]}...")
        print(f"{'='*80}\n")
        
        # åˆ†è¯åŒ–æç¤º
        tokenized_prompt = tokenizer(prompt, return_tensors="pt")
        
        # åˆ›å»º DataProto
        gen_batch = DataProto.from_dict({
            'input_ids': tokenized_prompt['input_ids'],
            'attention_mask': tokenized_prompt['attention_mask'],
            'position_ids': torch.arange(tokenized_prompt['input_ids'].shape[1], dtype=torch.long).unsqueeze(0)
        })
        gen_batch.meta_info = {'idx': [i]}
        
        # æ·»åŠ æç¤ºåˆ°å…ƒä¿¡æ¯ä»¥ä¾¿åç»­åˆ†æ
        gen_batch.non_tensor_batch = {
            'prompt': prompt,
            'reward_model': [{}]
        }
        
        try:
            # è¿è¡Œ ToT Rollout
            print(f"ğŸ”„ æ‰§è¡Œ Tree of Thought rollout...")
            start_time = time.time()
            
            # æ‰§è¡Œ rollout
            rollout_output = tot_agent.run_llm_loop(gen_batch, output_dir="./tot_validation_output", global_steps=0)
            
            end_time = time.time()
            rollout_time = end_time - start_time
            
            print(f"âœ… Rollout å®Œæˆ! è€—æ—¶: {rollout_time:.2f} ç§’")
            
            # æå–ç»“æœ
            if hasattr(rollout_output, 'meta_info'):
                # æå–è½¨è¿¹
                trajectories = rollout_output.meta_info.get('rollout_trajectory', [])
                if trajectories:
                    print(f"\nğŸŒŸ æ‰¾åˆ° {len(trajectories)} ä¸ªè½¨è¿¹.")
                    
                    # æ˜¾ç¤ºç¬¬ä¸€ä¸ªè½¨è¿¹
                    print(f"\nğŸŒŸ æœ€ä½³è½¨è¿¹:")
                    visualize_trajectory(trajectories[0])
                    
                    # é¢å¤–ä¿¡æ¯
                    tot_metrics = rollout_output.meta_info.get('metrics', {})
                    if tot_metrics:
                        for k, v in tot_metrics.items():
                            if k.startswith('tot_'):
                                print(f"ğŸ“Š {k}: {v}")
                else:
                    print("âŒ æ²¡æœ‰æ‰¾åˆ°è½¨è¿¹åœ¨å…ƒä¿¡æ¯ä¸­.")
                
                # æå–å¥–åŠ±
                if 'reward' in rollout_output.meta_info:
                    rewards = rollout_output.meta_info['reward']
                    print(f"ğŸ’° å¥–åŠ±: {rewards}")
                
                # æ›´å¤šç»†èŠ‚
                if debug:
                    print("\nğŸ” è°ƒè¯•ä¿¡æ¯:")
                    for key, value in rollout_output.meta_info.items():
                        if key not in ['rollout_trajectory']:
                            print(f"  - {key}: {value}")
            else:
                print("âŒ rollout_output æ²¡æœ‰ meta_info å±æ€§.")
            
            # æ”¶é›†ç»“æœ
            all_results.append({
                'prompt': prompt,
                'success': True,
                'time': rollout_time,
                'num_trajectories': len(trajectories) if 'trajectories' in locals() else 0,
                'tot_metrics': tot_metrics if 'tot_metrics' in locals() and tot_metrics else {},
            })
            
        except Exception as e:
            import traceback
            print(f"âŒ æ‰§è¡Œ rollout æ—¶å‡ºé”™: {e}")
            traceback.print_exc()
            
            all_results.append({
                'prompt': prompt,
                'success': False,
                'error': str(e),
            })
    
    # æ˜¾ç¤ºæ±‡æ€»ç»“æœ
    print(f"\n{'='*80}")
    print(f"ğŸ“Š æ±‡æ€»ç»“æœ:")
    print(f"{'='*80}")
    
    successes = sum(1 for r in all_results if r['success'])
    print(f"âœ… æˆåŠŸ: {successes}/{len(all_results)}")
    
    if successes > 0:
        avg_time = sum(r['time'] for r in all_results if r['success']) / successes
        print(f"â±ï¸ å¹³å‡æ‰§è¡Œæ—¶é—´: {avg_time:.2f} ç§’")
        
        avg_trajs = sum(r.get('num_trajectories', 0) for r in all_results if r['success']) / successes
        print(f"ğŸŒ² å¹³å‡è½¨è¿¹æ•°: {avg_trajs:.2f}")
    
    # ä¿å­˜ç»“æœåˆ°æ–‡ä»¶
    results_file = "tot_validation_results.json"
    with open(results_file, "w") as f:
        json.dump({
            'config': {
                'model_path': model_path,
                'env_name': env_name,
                'tot_strategy': tot_strategy,
                'tot_beam_width': tot_beam_width,
                'tot_branches': tot_branches,
                'max_turns': max_turns,
            },
            'results': all_results
        }, f, indent=2)
    
    print(f"ğŸ“‘ è¯¦ç»†ç»“æœå·²ä¿å­˜åˆ° {results_file}")
    
    # æ¸…ç†
    if not debug:
        ray.shutdown()
    
    return all_results


def main():
    parser = argparse.ArgumentParser(description="éªŒè¯ Tree of Thought Agent å®ç°")
    parser.add_argument("--model_path", type=str, default="./Qwen2.5-3B", help="æ¨¡å‹è·¯å¾„")
    parser.add_argument("--env_name", type=str, default="webshop", help="ç¯å¢ƒåç§°")
    parser.add_argument("--env_server_base", type=str, default="http://0.0.0.0", help="ç¯å¢ƒæœåŠ¡å™¨åŸºåœ°å€")
    parser.add_argument("--env_port", type=int, default=36001, help="ç¯å¢ƒæœåŠ¡å™¨ç«¯å£")
    parser.add_argument("--tot_strategy", type=str, default="BFS", choices=["BFS", "DFS"], help="ToT æœç´¢ç­–ç•¥")
    parser.add_argument("--tot_beam", type=int, default=3, help="ToT æŸå®½")
    parser.add_argument("--tot_branches", type=int, default=10, help="æœ€å¤§åˆ†æ”¯æ•°")
    parser.add_argument("--max_turns", type=int, default=5, help="æœ€å¤§å›åˆæ•°")
    parser.add_argument("--num_examples", type=int, default=2, help="è¦æµ‹è¯•çš„ç¤ºä¾‹æ•°é‡")
    parser.add_argument("--debug", action="store_true", help="å¯ç”¨é¢å¤–çš„è°ƒè¯•è¾“å‡º")
    
    args = parser.parse_args()
    
    # å±•å¼€è·¯å¾„ä¸­çš„æ³¢æµªå·
    if args.model_path.startswith("~"):
        args.model_path = os.path.expanduser(args.model_path)
    
    # å¦‚æœæ¨¡å‹è·¯å¾„æ˜¯ç›¸å¯¹è·¯å¾„ä¸”ä¸ä»¥./å¼€å¤´ï¼Œæ·»åŠ ./
    if not args.model_path.startswith('/') and not args.model_path.startswith('./'):
        args.model_path = './' + args.model_path
    
    validate_tot_agent(
        model_path=args.model_path,
        env_name=args.env_name,
        env_server_base=args.env_server_base,
        env_ports=[args.env_port],
        tot_strategy=args.tot_strategy,
        tot_beam_width=args.tot_beam,
        tot_branches=args.tot_branches,
        max_turns=args.max_turns,
        num_examples=args.num_examples,
        debug=args.debug
    )

if __name__ == "__main__":
    main()